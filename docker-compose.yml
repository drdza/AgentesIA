#version: "3.9"

services:

  llm-sql-inference-tf:
    container_name: llm-sql-inference-tf
    build:
      context: ./inference_sql_service
      dockerfile: Dockerfile
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/models/mistral
    volumes:
      - ./inference_sql_service/Mistral-7B-Instruct-v0.3.bak:/models/mistral
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['all']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ['disable']

  llm-sql-inference-tgi:
    container_name: llm-sql-inference
    image: ghcr.io/huggingface/text-generation-inference:3.3.2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['all']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    ports:
      - "8000:80"
    volumes:
       - ./inference_sql_service/Mistral-7B-Instruct-v0.3.bak:/model
    command:
      --model-id /model
      --max-input-length 3072
      --max-total-tokens 4096
    restart: unless-stopped
    profiles: ['sql-stack']

  llm-context-inference:
    container_name: llm-context-inference-microservice
    image: nvcr.io/nim/google/gemma-2-9b-it:1.4.0
    runtime: nvidia
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 30s
      timeout: 20s
      retries: 100
      start_period: 300s
    ports:
      - "8001:8000"
    volumes:
      - ./model_cache:/opt/nim/.cache
    shm_size: 32gb
    mem_limit: 64g
    memswap_limit: 64g
    user: "${USERID}"
    profiles: ['disable']

  llm-embeddings:
    container_name: embedding-microservice
    image: nvcr.io/nim/nvidia/nv-embedqa-e5-v5-pb24h2:1.2.0
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
    depends_on:
      llm-sql-inference-tgi:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/v1/health/ready"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 300s
    ports:
      - "9080:8000"
    volumes:
      - ./model_cache:/opt/nim/.cache
    shm_size: 16gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['all']
              capabilities: [gpu]
    user: "${USERID}"
    profiles: ['sql-stack']

  altheia_sql_backend:
    container_name: altheia_sql_backend
    build:
      context: ./altheia-sql
      dockerfile: backend/Dockerfile
    # image: altheia/sql_backend:latest
    ports:
      - "8010:8000"
    volumes:
      - ./altheia-sql/backend:/app/backend
      - ./altheia-sql/shared:/app/shared
      - ./altheia-sql/prompts:/app/prompts
      - ./altheia-sql/logs:/app/logs
      - ./altheia-sql/outputs:/app/outputs
    depends_on:
      - llm-sql-inference-tgi
    profiles: ['sql-stack']

  altheia_sql_frontend:
    container_name: altheia_sql_frontend
    build:
      context: ./altheia-sql
      dockerfile: frontend/Dockerfile
    # image: altheia/sql_frontend:latest
    ports:
      - "8510:8501"
    volumes:
      - ./altheia-sql/frontend:/app/frontend
      - ./altheia-sql/shared:/app/shared
      - ./altheia-sql/prompts:/app/prompts
      - ./altheia-sql/logs:/app/logs
      - ./altheia-sql/outputs:/app/outputs
    depends_on:
      - altheia_sql_backend
    profiles: ['sql-stack']

  etcd:
    container_name: etcd
    image: quay.io/coreos/etcd:v3.5.16
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - etcd_data:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ['sql-stack']

  minio:
    container_name: minio
    image: minio/minio:RELEASE.2024-05-01T01-11-10Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9011:9011"
      - "9010:9010"
    volumes:
      - minio_data:/data
      #- minio_data:/minio_data
    command: minio server /data --console-address ":9011" --address ":9010"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ['sql-stack']

  milvus:
    container_name: milvus
    image: milvusdb/milvus:v2.4.23
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9010
      KNOWHERE_GPU_MEM_POOL_SIZE: 2048;4096
      MINIO_BUCKET_NAME: milvus-bucket
    volumes:
      - milvus_data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    profiles: ['sql-stack']

volumes:
  milvus_data:
    name: milvus_data
  minio_data:
    name: minio_data
  etcd_data:
    name: etcd_data

networks:
  defualt:
    name: altheia_network